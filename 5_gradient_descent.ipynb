{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_gradient_descent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0OBdKM2MYe4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "np.random.seed(0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ETTc1EmOP_E"
      },
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class LinearRegressionSGD(BaseEstimator):\n",
        "    def __init__(self, epsilon=1e-6, max_steps=10000, w0=None, alpha=1e-8):\n",
        "        self.epsilon = epsilon\n",
        "        self.max_steps = max_steps\n",
        "        self.w0 = w0\n",
        "        self.alpha = alpha\n",
        "        self.w = None\n",
        "        self.w_history = []\n",
        "\n",
        "def fit(self, X, y):\n",
        "        l,d = X.shape \n",
        "        if self.w0 is None:\n",
        "            self.w0 = np.zeros(d)\n",
        "        self.w = self.w0\n",
        "        for step in range(self.max_steps):\n",
        "            self.w_history.append(self.w)\n",
        "            w_new = self.w - self.alpha * self.calc_gradient(X, y)\n",
        "            if (np.linalg.norm(w_new - self.w) < self.epsilon):\n",
        "                break\n",
        "            self.w = w_new\n",
        "        return self\n",
        "\n",
        "def predict(self, X):\n",
        "        if self.w is None:\n",
        "            raise Exception(\"Пока не обучена\")  \n",
        "        y_pred = np.dot(X, self.w)\n",
        "        return y_pred\n",
        "\n",
        "def calc_gradient(self, X, y):\n",
        "        l, d = X.shape\n",
        "        gradient = np.zeros((d, ))\n",
        "        indeces = np.random.randint(0, d, (10, ))\n",
        "        return (2/l) * np.dot(X.T,(np.dot(X, self.w) - y))"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}